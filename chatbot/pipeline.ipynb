{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c47df9",
   "metadata": {},
   "source": [
    "### Imports & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ee1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai\n",
    "# !pip install langchain_community\n",
    "# !pip install langchain_huggingface\n",
    "# !pip install gradio\n",
    "# !pip install rapidfuzz\n",
    "# !pip install pypdf\n",
    "# !pip install faiss-cpu\n",
    "# !pip install spacy\n",
    "# !pip install fuzzywuzzy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install django\n",
    "# !pip install django-cors-headers\n",
    "# !pip install djangorestframework\n",
    "# !pip install drf_spectacular\n",
    "# !pip install psycopg2\n",
    "# !python.exe -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8afbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MO\\Depi\\envai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MO\\Depi\\envai\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, os, re, ast\n",
    "import subprocess\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "from rapidfuzz import fuzz\n",
    "pdf_file = \"./res/Cli script guidebook.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b9b56",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a640a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name=\"gemini-1.5-pro\"):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=500,\n",
    "        google_api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770839c8",
   "metadata": {},
   "source": [
    "#### Load API KEY from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848913a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_path = './credentials/google_credentials.json'\n",
    "if not os.path.exists(api_key_path):\n",
    "    raise FileNotFoundError(f\"API key file not found at {api_key_path}. Please provide the correct path.\")\n",
    "\n",
    "with open(api_key_path, 'r') as f:\n",
    "    api_key = json.load(f)\n",
    "os.environ['GOOGLE_API_KEY'] = api_key.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ff185",
   "metadata": {},
   "source": [
    "### Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_file)\n",
    "pdf_docs = loader.load_and_split()\n",
    "\n",
    "with open(\"res/data_mapping.json\", \"r\") as f:\n",
    "    data_mapping = json.load(f)\n",
    "\n",
    "REFERENCE_KEYWORDS = list(data_mapping.keys())\n",
    "data_mapping_doc = Document(page_content='Default Arguments for each node:\\n'+ json.dumps(data_mapping))\n",
    "\n",
    "all_docs = pdf_docs  + [data_mapping_doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42781df",
   "metadata": {},
   "source": [
    "### Create Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61295ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=all_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19f4b3",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c696a0",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c692a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def combine_inputs(input_dict, retriever, cur_iter=0):\n",
    "    question = input_dict[\"question\"]\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    return {\n",
    "        \"context\": format_docs(retrieved_docs),\n",
    "        \"question\": question,\n",
    "        \"cur_iter\": cur_iter\n",
    "    }\n",
    "\n",
    "def parse_command_list(output: str):\n",
    "    pattern = r\"\\[(.*?)\\]\"\n",
    "    matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        output = f\"[{matches[0]}]\"\n",
    "\n",
    "    try:\n",
    "        command_list = ast.literal_eval(output.strip())\n",
    "        return command_list if isinstance(command_list, list) else [command_list]\n",
    "    except Exception as e:\n",
    "        return [f\"Failed to parse: {e}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0452d50",
   "metadata": {},
   "source": [
    "#### LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dadf03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template1 = \"\"\"You are an expert assistant trained to extract exact command-line instructions from a user guide.\n",
    "\n",
    "You are given the folliwing information:\n",
    "- A user guide that contains command-line instructions for a CLI tool.\n",
    "- all possible values for node_name\n",
    "- mapping of node names to their default <args>.\n",
    "\n",
    "Reference Information:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Carefully read the Reference Information, node names, and default arguments for each node.\n",
    "- If user didn't provide any value for the arguments, use the default values from the mapping using this.\n",
    "- Return the CLI command(s) that are relevant to the user's query.\n",
    "- Format your response as a raw Python list: ['command1', 'command2', ...]\n",
    "- Do NOT include any explanations, comments, or formatting outside the list.\n",
    "- If no command matches the query, return an empty list: []\n",
    "- If user provided arguments make it in json format. e.g. {{'arg1': 'value1', 'arg2': 'value2'}}\n",
    "- The output MUST be ordered in the same order as the input.\n",
    "\n",
    "Example Query:\n",
    "I want to make a logistic regression model and make a project with id 3.\n",
    "\n",
    "Expected Response:\n",
    "[\n",
    "    'make logistic_regression {{\"model_name\":\"logistic_regression\",\"model_type\":\"linear_models\",\"task\":\"classification\"}}',\n",
    "    'create_project 3'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough()\n",
    "    | (lambda x: combine_inputs(x, retriever, cur_iter=0))\n",
    "    | prompt1\n",
    "    | get_llm(model_name=\"gemini-2.0-flash\")\n",
    "    | StrOutputParser()\n",
    "    | parse_command_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177048a",
   "metadata": {},
   "source": [
    "### Keyword Extraction (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daee2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_extractor = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "keyword_embeddings = name_extractor.encode(REFERENCE_KEYWORDS, convert_to_tensor=True)\n",
    "\n",
    "def extract_keywords_hybrid(user_input: str, reference_keywords, top_n, transformer_thresh=0.7, fuzzy_thresh=80):\n",
    "    keyword_embeddings = name_extractor.encode(reference_keywords, convert_to_tensor=True)\n",
    "    doc = nlp(user_input.lower())\n",
    "    phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    matched_keywords = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase_embedding = name_extractor.encode(phrase, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(phrase_embedding, keyword_embeddings)[0]\n",
    "\n",
    "        for idx, score in enumerate(cosine_scores):\n",
    "            if score >= transformer_thresh:\n",
    "                if reference_keywords[idx] not in matched_keywords:\n",
    "                    matched_keywords.append(reference_keywords[idx])\n",
    "\n",
    "        # === Fuzzy matching fallback ===\n",
    "        for keyword in reference_keywords:\n",
    "            if fuzz.ratio(phrase, keyword.replace(\"_\", \" \")) >= fuzzy_thresh:\n",
    "                if keyword not in matched_keywords:\n",
    "                    matched_keywords.append(keyword)\n",
    "    top_n = min(top_n, len(matched_keywords))\n",
    "    best_match = matched_keywords.copy()\n",
    "    if len(matched_keywords) > top_n:\n",
    "        best_match = sorted(matched_keywords, key=lambda x: fuzz.ratio(user_input, x), reverse=True)[:top_n]\n",
    "    matched_keywords = list(filter(lambda x: x in best_match, matched_keywords))\n",
    "    return matched_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef6191",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d4dd0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_extractor(names, reference_keywords, data_mapping):\n",
    "    result = []\n",
    "    for name in names:\n",
    "        if name in reference_keywords and name in data_mapping:\n",
    "            # node_type = mapping[name]\n",
    "            args_str = json.dumps(data_mapping[name], separators=(',', ':'))\n",
    "            result.append(args_str)\n",
    "    return result\n",
    "\n",
    "def replace_args(commands, replacements, names, reference_keywords):\n",
    "\n",
    "    c = 0\n",
    "    for i, command in enumerate(commands):\n",
    "        if \"<args>\" in command:\n",
    "            if names[c] in reference_keywords:\n",
    "                commands[i] = command.replace(\"<args>\", replacements[c])\n",
    "                c += 1\n",
    "\n",
    "    return commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"You are great at extracting suitable commands from user queries.\n",
    "\n",
    "# You are given:\n",
    "# - A mapping of node names to their default arguments.\n",
    "# - A user's natural language query.\n",
    "# - An empty list of commands to be filled.\n",
    "\n",
    "# Your task:\n",
    "# 1. Carefully read the user query.\n",
    "# 2. Identify any mentioned node names, even if they are misspelled or partially named (e.g., \"logistice\" → \"logistic_regression\").\n",
    "# 3. For each recognized node:\n",
    "#    - Retrieve its default arguments from the mapping.\n",
    "#    - If the user provided new values for any arguments, update the default arguments accordingly.\n",
    "# 4. Format a command for each node in the following format:\n",
    "#    - `\"make <node_name> <arg1>=<value1> <arg2>=<value2> ...\"`\n",
    "#    - Skip arguments if they are empty.\n",
    "# 5. Return your response as a **raw Python list**, like: `['make node1 arg1=val1', 'make node2']`\n",
    "# 6. If no node matches the query, return an empty list: `[]`\n",
    "# 7. Do NOT explain or add any extra text. Only output the list.\n",
    "\n",
    "# Reference Mapping:\n",
    "# {context}\n",
    "\n",
    "# User Query:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "# prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d700f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_chain = (\n",
    "#     RunnablePassthrough()\n",
    "#     | (lambda x: combine_inputs(x, mapping_retriever))\n",
    "#     | prompt\n",
    "#     | get_llm(model_name=\"gemini-2.0-flash\")\n",
    "#     | StrOutputParser()\n",
    "#     | parse_command_list\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a7552c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_pipeline(documents, prompt, question, cur_iter, model_name=\"gemini-2.0-flash\"):\n",
    "    model = get_llm(model_name=model_name)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    retriever = FAISS.from_documents(documents=documents, embedding=embeddings).as_retriever()\n",
    "    rag_chain = (\n",
    "        RunnablePassthrough()\n",
    "        | (lambda x: combine_inputs(x, retriever, cur_iter))\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "        # | parse_command_list\n",
    "    )\n",
    "    return rag_chain.invoke({\"question\":question, \"cur_iter\": cur_iter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f240651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "[\n",
      "    'create_project 3',\n",
      "    'make logistic_regression {\"model_name\":\"logistic_regression\",\"model_type\":\"linear_models\",\"task\":\"classification\"}',\n",
      "    'make standard_scaler {}'\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"I want to create a project that has id of 3, use logistice regression model and use standard scaler\"\n",
    "print(RAG_pipeline(all_docs, prompt1, question, cur_iter=0, model_name=\"gemini-2.0-flash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddbbe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps for each pipeline \n",
      " \n",
      "Model fitting with a dataset \n",
      "- make <model_name> <args>  \n",
      "- make data_loader <args>  \n",
      "- show data_loader <data_loader_id> 1  \n",
      "- show data_loader <data_loader_id> 2  \n",
      "- make model_fitter {“model”:<model_id>,”X”:data_loader_X_id,”y”:data_loader_y_id}\n"
     ]
    }
   ],
   "source": [
    "file1 = './res/steps.pdf'\n",
    "file2 = pdf_file\n",
    "loader = PyPDFLoader(file1)\n",
    "steps_docs = loader.load_and_split()\n",
    "new = steps_docs + pdf_docs + [data_mapping_doc]\n",
    "print(new[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc38576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'message': 'data loaded: iris: y', 'node_id': 2889729083600}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_id_message(json_str):\n",
    "    try:\n",
    "        json_obj = eval(json_str)\n",
    "        message = json_obj.get(\"message\")\n",
    "        node_id = json_obj.get(\"node_id\")\n",
    "        json_obj = {\"message\": message, \"node_id\": node_id}\n",
    "        return f'{json_obj}'\n",
    "    except:\n",
    "        return json_str\n",
    "extract_id_message(str({\"node_id\": 2889729083600,\"node_name\": \"data_loader_y\",\"message\": \"data loaded: iris: y\",\"params\": {},\"task\": \"load_data\",\"node_type\": \"loader\",\"children\": [],\"location_x\": 0.0,\"location_y\": 0.0,\"input_ports\": [],\"output_ports\": [],\"project\": 1,\"node_data\": \"C:\\\\Users\\\\a1mme\\\\OneDrive\\\\Desktop\\\\MO\\\\test_grad\\\\project\\\\core\\\\saved\\\\other\\\\data_loader_2889729083600.pkl\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b435837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('app.py'), '..')))\n",
    "from cli.call_cli import call_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template2 = \"\"\"You are an expert assistant who takes input from user and has a guide book which helps you, also you have\n",
    "a response from the api that is stored into your corpus. \n",
    "\n",
    "You are given the following information in your corpus:\n",
    "- steps to follow (follow them one by one).\n",
    "- A user guide that contains command-line instructions for a CLI tool.\n",
    "- A response from api, this response is given in your reference information.\n",
    "- A mapping of node names to their default arguments.\n",
    "- and the current iteration number.\n",
    "\n",
    "Reference Information:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Current Iteration:\n",
    "{cur_iter}\n",
    "\n",
    "Instructions:\n",
    "- Carefully read the Reference Information, user query.\n",
    "- Follow the steps that is given to you in the reference information one by one.\n",
    "- take the response from API (which is also stored in your corpus) to fill in the values that needs to be filled e.g. <model_id>.\n",
    "- If user didn't provide any value for the arguments, use the default values from the mapping using this.\n",
    "- Return the CLI command(s) that are relevant to the user's query.\n",
    "- Show only the current command. You can know that by the current iteration number.\n",
    "- Format the output as string.\n",
    "- Do NOT include any explanations, comments, or formatting outside the string.\n",
    "- Monitor the message of the api response and determine whether there is an error or not.\n",
    "\n",
    "Note The Following: \n",
    "- in example '$' means that it is an extracted value from your corpus.\n",
    "- There are some nodes that has multiple output, so to specify the output channel, use the number of the channel. e.g. 'show data_loader $data_loader_id 1' means that you are showing the first channel of data loader node which is X,\n",
    "\n",
    "example query:\n",
    "I want to fit knn on diabetes dataset\n",
    "\n",
    "Expected Response:\n",
    "[\n",
    "    'make knn_classifier {{\"model_name\":\"knn_classifier\",\"task\":\"classification\",\"model_type\":\"knn\",\"params\":{{\"n_neighbors\": 5}}}}',\n",
    "    'make data_loader {{\"params\":{{\"dataset_name\": \"iris\"}}}}',\n",
    "    'show data_loader data_loader_id 1',\n",
    "    'show data_loader data_loader_id 2',\n",
    "    'make model_fitter {{'model': model_id, 'X': X_id, 'y': y_id}}'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Output : \n",
      "['make logistic_regression {\"model_name\": \"logistic_regression\", \"task\": \"classification\", \"model_type\": \"linear_models\", \"params\": {\"penalty\": \"l2\", \"C\": 1.0}}']\n",
      "Subprocess completed successfully.\n",
      "API Output 0: \n",
      "{'node_id': 1809860798528, 'node_name': 'logistic_regression', 'message': 'Model logistic_regression created', 'node_data': 'C:\\\\Users\\\\a1mme\\\\OneDrive\\\\Desktop\\\\MO\\\\test_grad\\\\project\\\\core\\\\saved\\\\model\\\\logistic_regression_1809860798528.pkl', 'params': {'C': 1.0, 'penalty': 'l2'}, 'task': 'classification', 'node_type': 'linear_models', 'project_id': 1, 'children': [], 'location_x': 0.0, 'location_y': 0.0, 'input_ports': [], 'output_ports': []}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# question = \"I want to fit logistic regression model on iris dataset\"\n",
    "api_response = ' '\n",
    "docs = new.copy()\n",
    "docs += [Document(page_content='')]\n",
    "i = 1\n",
    "manual_mode = 1 if input(\"Select Mode 1 for manual, 0 for auto: \") == '1' else 0\n",
    "question = input(\"User Query: \") if not manual_mode else ''      # One question\n",
    "while True:\n",
    "    try:\n",
    "        if manual_mode:  # This mode makes the user be able to add any comment to the chatbot.\n",
    "            \"\"\"\n",
    "            Manual Mode: \n",
    "            Where the chatbot Receive Prompts from user and iteracts with cli.\n",
    "            \"\"\"\n",
    "            question = input(\"User Query: \")\n",
    "            if question.strip().lower() in (\"exit\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            rag_out = RAG_pipeline(all_docs, prompt1, question, cur_iter=0, model_name=\"gemini-2.0-flash\")\n",
    "            rag_out = parse_command_list(rag_out)\n",
    "            print(f\"RAG Output : \\n\"+str(rag_out))\n",
    "\n",
    "            for a, out in enumerate(rag_out):\n",
    "                api_response = call_script(out)\n",
    "                print(f\"API Output {a}: \\n\"+str(api_response))\n",
    "            print('-'*50)\n",
    "\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Automatic Mode:\n",
    "            Where User only Specify what he want, and the chatbot will take care of the rest.\n",
    "            \"\"\"\n",
    "            if not api_response:\n",
    "                break \n",
    "            docs[-1].page_content += '\\n' + str(api_response)\n",
    "            rag_out = RAG_pipeline(docs, prompt2, question, cur_iter=i, model_name=\"gemini-2.0-flash\")\n",
    "            rag_out = rag_out.strip().strip(\"'\").strip('\"')\n",
    "\n",
    "            print(f\"RAG Output {i}: \",rag_out)\n",
    "            # api_response = extract_id_message(call_script(rag_out))       # UNCOMMENT to make it interact with cli\n",
    "            api_response = extract_id_message(input(\"API Response: \"))      # COMMENT this part, it is made for debugging.\n",
    "            print(f\"API Output {i}: \",api_response)\n",
    "            print('-'*50)\n",
    "            i +=1\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "# print(rag_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da417c8",
   "metadata": {},
   "source": [
    "### Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c9592497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG:  ['create_project 3', 'make logistic_regression <args>', 'make standard_scaler <args>']\n",
      "Extracted nodes' names:  ['logistic_regression', 'standard_scaler']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['create_project 3',\n",
       " 'make logistic_regression {\"model_name\":\"logistic_regression\",\"task\":\"classification\",\"model_type\":\"linear_models\",\"params\":{\"penalty\":\"l2\",\"C\":1.0}}',\n",
       " 'make standard_scaler {\"preprocessor_name\":\"standard_scaler\",\"preprocessor_type\":\"scaler\",\"params\":{\"with_mean\":true,\"with_std\":true}}']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"I want to create a project that has id of 3, use logistice regression model and use standard scaler\"\n",
    "# RAG output\n",
    "rag_output = rag_chain.invoke({\"question\": question})\n",
    "print(\"RAG: \",rag_output)\n",
    "# # Extract command names\n",
    "names = extract_keywords_hybrid(question, REFERENCE_KEYWORDS, top_n=len(rag_output))\n",
    "print(\"Extracted nodes' names: \",names)\n",
    "# # Extract args and types\n",
    "args_list = args_extractor(names, REFERENCE_KEYWORDS, data_mapping)\n",
    "# # Parse RAG and replace placeholders\n",
    "final_output = replace_args(rag_output, args_list, names, REFERENCE_KEYWORDS)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bdaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
