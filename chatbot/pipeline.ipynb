{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c47df9",
   "metadata": {},
   "source": [
    "### Imports & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ee1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai\n",
    "# !pip install langchain_community\n",
    "# !pip install langchain_huggingface\n",
    "# !pip install gradio\n",
    "# !pip install rapidfuzz\n",
    "# !pip install pypdf\n",
    "# !pip install faiss-cpu\n",
    "# !pip install spacy\n",
    "# !pip install fuzzywuzzy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install django\n",
    "# !pip install django-cors-headers\n",
    "# !pip install djangorestframework\n",
    "# !pip install drf_spectacular\n",
    "# !pip install psycopg2\n",
    "# !python.exe -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8afbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MO\\Depi\\envai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MO\\Depi\\envai\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, os, re, ast\n",
    "import subprocess\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "from rapidfuzz import fuzz\n",
    "pdf_file = \"./res/Cli script guidebook.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b9b56",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a640a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name=\"gemini-1.5-pro\"):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=500,\n",
    "        google_api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770839c8",
   "metadata": {},
   "source": [
    "#### Load API KEY from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848913a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_path = './credentials/google_credentials.json'\n",
    "if not os.path.exists(api_key_path):\n",
    "    raise FileNotFoundError(f\"API key file not found at {api_key_path}. Please provide the correct path.\")\n",
    "\n",
    "with open(api_key_path, 'r') as f:\n",
    "    api_key = json.load(f)\n",
    "os.environ['GOOGLE_API_KEY'] = api_key.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ff185",
   "metadata": {},
   "source": [
    "### Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_file)\n",
    "pdf_docs = loader.load_and_split()\n",
    "\n",
    "with open(\"res/data_mapping.json\", \"r\") as f:\n",
    "    data_mapping = json.load(f)\n",
    "\n",
    "REFERENCE_KEYWORDS = list(data_mapping.keys())\n",
    "data_mapping_doc = Document(page_content='Default Arguments for each node:\\n'+ json.dumps(data_mapping))\n",
    "\n",
    "all_docs = pdf_docs  + [data_mapping_doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42781df",
   "metadata": {},
   "source": [
    "### Create Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61295ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=all_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19f4b3",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c696a0",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c692a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def combine_inputs(input_dict, retriever, cur_iter=0):\n",
    "    question = input_dict[\"question\"]\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    return {\n",
    "        \"context\": format_docs(retrieved_docs),\n",
    "        \"question\": question,\n",
    "        \"cur_iter\": cur_iter\n",
    "    }\n",
    "\n",
    "def parse_command_list(output: str):\n",
    "    pattern = r\"\\[(.*?)\\]\"\n",
    "    matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        output = f\"[{matches[0]}]\"\n",
    "\n",
    "    try:\n",
    "        command_list = ast.literal_eval(output.strip())\n",
    "        return command_list if isinstance(command_list, list) else [command_list]\n",
    "    except Exception as e:\n",
    "        return [f\"Failed to parse: {e}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0452d50",
   "metadata": {},
   "source": [
    "#### LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template1 = \"\"\"You are an expert assistant trained to extract exact command-line instructions from a user guide.\n",
    "\n",
    "You are given the folliwing information:\n",
    "- A user guide that contains command-line instructions for a CLI tool.\n",
    "- all possible values for node_name\n",
    "- mapping of node names to their default <args>.\n",
    "\n",
    "Reference Information:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Carefully read the Reference Information, node names, and default arguments for each node.\n",
    "- If user didn't provide any value for the arguments, use the default values from the mapping using this.\n",
    "- Return the CLI command(s) that are relevant to the user's query.\n",
    "- Format your response as a raw Python list: ['command1', 'command2', ...]\n",
    "- Do NOT include any explanations, comments, or formatting outside the list.\n",
    "- If no command matches the query, return an empty list: []\n",
    "- If user provided arguments make it in json format. e.g. {{'arg1': 'value1', 'arg2': 'value2'}}\n",
    "- The output MUST be ordered in the same order as the input.\n",
    "\n",
    "Example Query:\n",
    "I want to make a logistic regression model and make a project with id 3.\n",
    "\n",
    "Expected Response:\n",
    "[\n",
    "    'make logistic_regression {{}}',\n",
    "    'create_project 3'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough()\n",
    "    | (lambda x: combine_inputs(x, retriever, cur_iter=0))\n",
    "    | prompt1\n",
    "    | get_llm(model_name=\"gemini-2.0-flash\")\n",
    "    | StrOutputParser()\n",
    "    | parse_command_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef6191",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a7552c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_pipeline(documents, prompt, question, cur_iter, model_name=\"gemini-2.0-flash\"):\n",
    "    model = get_llm(model_name=model_name)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    retriever = FAISS.from_documents(documents=documents, embedding=embeddings).as_retriever()\n",
    "    rag_chain = (\n",
    "        RunnablePassthrough()\n",
    "        | (lambda x: combine_inputs(x, retriever, cur_iter))\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "        # | parse_command_list\n",
    "    )\n",
    "    return rag_chain.invoke({\"question\":question, \"cur_iter\": cur_iter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f240651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "[\n",
      "    'create_project 3',\n",
      "    'make logistic_regression {\"model_name\":\"logistic_regression\",\"model_type\":\"linear_models\",\"task\":\"classification\"}',\n",
      "    'make standard_scaler {}'\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"I want to create a project that has id of 3, use logistice regression model and use standard scaler\"\n",
    "print(RAG_pipeline(all_docs, prompt1, question, cur_iter=0, model_name=\"gemini-2.0-flash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddbbe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps for each pipeline \n",
      " \n",
      "Model fitting with a dataset \n",
      "- make <model_name> <args>  \n",
      "- make data_loader <args>  \n",
      "- show data_loader <data_loader_id> 1  \n",
      "- show data_loader <data_loader_id> 2  \n",
      "- make model_fitter {“model”:<model_id>,”X”:data_loader_X_id,”y”:data_loader_y_id}\n"
     ]
    }
   ],
   "source": [
    "file1 = './res/steps.pdf'\n",
    "file2 = pdf_file\n",
    "loader = PyPDFLoader(file1)\n",
    "steps_docs = loader.load_and_split()\n",
    "new = steps_docs + pdf_docs + [data_mapping_doc]\n",
    "print(new[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc38576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'message': 'data loaded: iris: y', 'node_id': 2889729083600}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_id_message(json_str):\n",
    "    try:\n",
    "        json_obj = eval(json_str)\n",
    "        message = json_obj.get(\"message\")\n",
    "        node_id = json_obj.get(\"node_id\")\n",
    "        json_obj = {\"message\": message, \"node_id\": node_id}\n",
    "        return f'{json_obj}'\n",
    "    except:\n",
    "        return json_str\n",
    "extract_id_message(str({\"node_id\": 2889729083600,\"node_name\": \"data_loader_y\",\"message\": \"data loaded: iris: y\",\"params\": {},\"task\": \"load_data\",\"node_type\": \"loader\",\"children\": [],\"location_x\": 0.0,\"location_y\": 0.0,\"input_ports\": [],\"output_ports\": [],\"project\": 1,\"node_data\": \"C:\\\\Users\\\\a1mme\\\\OneDrive\\\\Desktop\\\\MO\\\\test_grad\\\\project\\\\core\\\\saved\\\\other\\\\data_loader_2889729083600.pkl\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b435837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('app.py'), '..')))\n",
    "from cli.call_cli import call_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template2 = \"\"\"You are an expert assistant who takes input from user and has a guide book which helps you, also you have\n",
    "a response from the api that is stored into your corpus. \n",
    "\n",
    "You are given the following information in your corpus:\n",
    "- steps to follow (follow them one by one).\n",
    "- A user guide that contains command-line instructions for a CLI tool.\n",
    "- A response from api, this response is given in your reference information.\n",
    "- A mapping of node names to their default arguments.\n",
    "- and the current iteration number.\n",
    "\n",
    "Reference Information:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Current Iteration:\n",
    "{cur_iter}\n",
    "\n",
    "Instructions:\n",
    "- Carefully read the Reference Information, user query.\n",
    "- Follow the steps that is given to you in the reference information one by one.\n",
    "- take the response from API (which is also stored in your corpus) to fill in the values that needs to be filled e.g. <model_id>.\n",
    "- If user didn't provide any value for the arguments, use the default values from the mapping using this.\n",
    "- Return the CLI command(s) that are relevant to the user's query.\n",
    "- Show only the current command. You can know that by the current iteration number.\n",
    "- Format the output as string.\n",
    "- Do NOT include any explanations, comments, or formatting outside the string.\n",
    "- Monitor the message of the api response and determine whether there is an error or not.\n",
    "\n",
    "Note The Following: \n",
    "- in example '$' means that it is an extracted value from your corpus.\n",
    "- There are some nodes that has multiple output, so to specify the output channel, use the number of the channel. e.g. 'show data_loader $data_loader_id 1' means that you are showing the first channel of data loader node which is X,\n",
    "\n",
    "example query:\n",
    "I want to fit knn on diabetes dataset\n",
    "\n",
    "Expected Response:\n",
    "[\n",
    "    'make knn_classifier {{\"model_name\":\"knn_classifier\",\"task\":\"classification\",\"model_type\":\"knn\",\"params\":{{\"n_neighbors\": 5}}}}',\n",
    "    'make data_loader {{\"params\":{{\"dataset_name\": \"iris\"}}}}',\n",
    "    'show data_loader data_loader_id 1',\n",
    "    'show data_loader data_loader_id 2',\n",
    "    'make model_fitter {{'model': model_id, 'X': X_id, 'y': y_id}}'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Output : \n",
      "['make logistic_regression {\"model_name\": \"logistic_regression\", \"task\": \"classification\", \"model_type\": \"linear_models\", \"params\": {\"penalty\": \"l2\", \"C\": 1.0}}']\n",
      "Subprocess completed successfully.\n",
      "API Output 0: \n",
      "{'node_id': 1809860798528, 'node_name': 'logistic_regression', 'message': 'Model logistic_regression created', 'node_data': 'C:\\\\Users\\\\a1mme\\\\OneDrive\\\\Desktop\\\\MO\\\\test_grad\\\\project\\\\core\\\\saved\\\\model\\\\logistic_regression_1809860798528.pkl', 'params': {'C': 1.0, 'penalty': 'l2'}, 'task': 'classification', 'node_type': 'linear_models', 'project_id': 1, 'children': [], 'location_x': 0.0, 'location_y': 0.0, 'input_ports': [], 'output_ports': []}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# question = \"I want to fit logistic regression model on iris dataset\"\n",
    "api_response = ' '\n",
    "docs = new.copy()\n",
    "docs += [Document(page_content='')]\n",
    "i = 1\n",
    "manual_mode = 1 if input(\"Select Mode 1 for manual, 0 for auto: \") == '1' else 0\n",
    "question = input(\"User Query: \") if not manual_mode else ''      # One question\n",
    "while True:\n",
    "    try:\n",
    "        if manual_mode:  # This mode makes the user be able to add any comment to the chatbot.\n",
    "            \"\"\"\n",
    "            Manual Mode: \n",
    "            Where the chatbot Receive Prompts from user and iteracts with cli.\n",
    "            \"\"\"\n",
    "            question = input(\"User Query: \")\n",
    "            if question.strip().lower() in (\"exit\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            rag_out = RAG_pipeline(all_docs, prompt1, question, cur_iter=0, model_name=\"gemini-2.0-flash\")\n",
    "            rag_out = parse_command_list(rag_out)\n",
    "            print(f\"RAG Output : \\n\"+str(rag_out))\n",
    "\n",
    "            for a, out in enumerate(rag_out):\n",
    "                api_response = call_script(out)\n",
    "                print(f\"API Output {a}: \\n\"+str(api_response))\n",
    "            print('-'*50)\n",
    "\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Automatic Mode:\n",
    "            Where User only Specify what he want, and the chatbot will take care of the rest.\n",
    "            \"\"\"\n",
    "            if not api_response:\n",
    "                break \n",
    "            docs[-1].page_content += '\\n' + str(api_response)\n",
    "            rag_out = RAG_pipeline(docs, prompt2, question, cur_iter=i, model_name=\"gemini-2.0-flash\")\n",
    "            rag_out = rag_out.strip().strip(\"'\").strip('\"')\n",
    "\n",
    "            print(f\"RAG Output {i}: \",rag_out)\n",
    "            # api_response = extract_id_message(call_script(rag_out))       # UNCOMMENT to make it interact with cli\n",
    "            api_response = extract_id_message(input(\"API Response: \"))      # COMMENT this part, it is made for debugging.\n",
    "            print(f\"API Output {i}: \",api_response)\n",
    "            print('-'*50)\n",
    "            i +=1\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "# print(rag_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da417c8",
   "metadata": {},
   "source": [
    "### Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9592497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG:  ['create_project 3', 'make logistic_regression <args>', 'make standard_scaler <args>']\n",
      "Extracted nodes' names:  ['logistic_regression', 'standard_scaler']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['create_project 3',\n",
       " 'make logistic_regression {\"model_name\":\"logistic_regression\",\"task\":\"classification\",\"model_type\":\"linear_models\",\"params\":{\"penalty\":\"l2\",\"C\":1.0}}',\n",
       " 'make standard_scaler {\"preprocessor_name\":\"standard_scaler\",\"preprocessor_type\":\"scaler\",\"params\":{\"with_mean\":true,\"with_std\":true}}']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"I want to create a project that has id of 3, use logistice regression model and use standard scaler\"\n",
    "# RAG output\n",
    "rag_output = rag_chain.invoke({\"question\": question})\n",
    "print(\"RAG: \",rag_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bdaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
