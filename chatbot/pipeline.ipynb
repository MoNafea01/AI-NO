{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c47df9",
   "metadata": {},
   "source": [
    "### Imports & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6ee1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai\n",
    "# !pip install langchain_community\n",
    "# !pip install langchain_huggingface\n",
    "# !pip install gradio\n",
    "# !pip install rapidfuzz\n",
    "# !pip install pypdf\n",
    "# !pip install faiss-cpu\n",
    "# !pip install spacy\n",
    "# !pip install fuzzywuzzy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install django\n",
    "# !pip install django-cors-headers\n",
    "# !pip install djangorestframework\n",
    "# !pip install drf_spectacular\n",
    "# !pip install psycopg2\n",
    "# !python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8afbe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, ast\n",
    "import subprocess\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "from rapidfuzz import fuzz\n",
    "pdf_file = \"./res/Cli script guidebook.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b9b56",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a640a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name=\"gemini-1.5-pro\"):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=500,\n",
    "        google_api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770839c8",
   "metadata": {},
   "source": [
    "#### Load API KEY from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848913a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_path = './credentials/google_credentials.json'\n",
    "if not os.path.exists(api_key_path):\n",
    "    raise FileNotFoundError(f\"API key file not found at {api_key_path}. Please provide the correct path.\")\n",
    "\n",
    "with open(api_key_path, 'r') as f:\n",
    "    api_key = json.load(f)\n",
    "os.environ['GOOGLE_API_KEY'] = api_key.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ff185",
   "metadata": {},
   "source": [
    "### Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56dac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_file)\n",
    "pdf_docs = loader.load_and_split()\n",
    "\n",
    "# Load mapping\n",
    "with open(\"res/mapping.json\", \"r\") as f:\n",
    "    raw_mapping = json.load(f)\n",
    "\n",
    "with open(\"res/data_mapping.json\", \"r\") as f:\n",
    "    data_mapping = json.load(f)\n",
    "\n",
    "# Format mapping as Document\n",
    "mapping_text = \"\\n\".join(f\"{k} -> {v}\" for k, v in raw_mapping.items())\n",
    "mapping_doc = Document(page_content=mapping_text)\n",
    "\n",
    "all_docs = pdf_docs + [mapping_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42781df",
   "metadata": {},
   "source": [
    "### Create Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61295ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=all_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19f4b3",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c692a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def combine_inputs(input_dict):\n",
    "    question = input_dict[\"question\"]\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    return {\n",
    "        \"context\": format_docs(retrieved_docs),\n",
    "        \"question\": question\n",
    "    }\n",
    "\n",
    "template = \"\"\"You are an expert assistant trained to extract exact command-line instructions from a user guide.\n",
    "\n",
    "You are also provided with a node-to-subcommand mapping reference (e.g., \"logistic_regression\" → \"model\", \"standard_scaler\" → \"preprocessor\"). Use this mapping only to determine the correct value for <node_name> in the CLI command syntax.\n",
    "\n",
    "Reference Information:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Carefully read the Reference Information, including any node-to-subcommand mappings and CLI usage documentation.\n",
    "- Return the CLI command(s) that are relevant to the user's query with the node name in the node-to subcommand mapping.\n",
    "- Use the mapping to replace <node_name> with its corresponding type (e.g., model, preprocessor, etc.)\n",
    "- IMPORTANT: Never modify or replace <args>. Keep <args> exactly as it is shown.\n",
    "- Format your response as a raw Python list: ['command1', 'command2', ...]\n",
    "- Do NOT include any explanations, comments, or formatting outside the list.\n",
    "- If no command matches the query, return an empty list: []\n",
    "- The output MUST be ordered in the same order as the input.\n",
    "\n",
    "Example Query:\n",
    "I want to make a logistic regression model and make a project.\n",
    "\n",
    "Expected Response:\n",
    "[\n",
    "    'make model <args>',\n",
    "    'create_project <project_id>'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough()\n",
    "    | combine_inputs\n",
    "    | prompt\n",
    "    | get_llm(model_name=\"gemini-2.0-flash\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177048a",
   "metadata": {},
   "source": [
    "### Keyword Extraction (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daee2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_extractor = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "REFERENCE_KEYWORDS = list(raw_mapping.keys())\n",
    "keyword_embeddings = name_extractor.encode(REFERENCE_KEYWORDS, convert_to_tensor=True)\n",
    "\n",
    "def extract_keywords_hybrid(user_input: str, reference_keywords, top_n, transformer_thresh=0.7, fuzzy_thresh=80):\n",
    "    keyword_embeddings = name_extractor.encode(reference_keywords, convert_to_tensor=True)\n",
    "    doc = nlp(user_input.lower())\n",
    "    phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    matched_keywords = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase_embedding = name_extractor.encode(phrase, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(phrase_embedding, keyword_embeddings)[0]\n",
    "\n",
    "        for idx, score in enumerate(cosine_scores):\n",
    "            if score >= transformer_thresh:\n",
    "                if reference_keywords[idx] not in matched_keywords:\n",
    "                    matched_keywords.append(reference_keywords[idx])\n",
    "\n",
    "        # === Fuzzy matching fallback ===\n",
    "        for keyword in reference_keywords:\n",
    "            if fuzz.ratio(phrase, keyword.replace(\"_\", \" \")) >= fuzzy_thresh:\n",
    "                if keyword not in matched_keywords:\n",
    "                    matched_keywords.append(keyword)\n",
    "    top_n = min(top_n, len(matched_keywords))\n",
    "    best_match = matched_keywords.copy()\n",
    "    if len(matched_keywords) > top_n:\n",
    "        best_match = sorted(matched_keywords, key=lambda x: fuzz.ratio(user_input, x), reverse=True)[:top_n]\n",
    "    matched_keywords = list(filter(lambda x: x in best_match, matched_keywords))\n",
    "    return matched_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef6191",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d4dd0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_command_list(output: str):\n",
    "    pattern = r\"\\[(.*?)\\]\"\n",
    "    matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        output = f\"[{matches[0]}]\"\n",
    "\n",
    "    try:\n",
    "        command_list = ast.literal_eval(output.strip())\n",
    "        return command_list if isinstance(command_list, list) else [command_list]\n",
    "    except Exception as e:\n",
    "        return [f\"Failed to parse: {e}\"]\n",
    "\n",
    "def args_extractor(names, mapping, data_mapping):\n",
    "    result = []\n",
    "    for name in names:\n",
    "        if name in mapping and name in data_mapping:\n",
    "            # node_type = mapping[name]\n",
    "            args_str = json.dumps(data_mapping[name], separators=(',', ':'))\n",
    "            result.append(args_str)\n",
    "    return result\n",
    "\n",
    "def replace_args(commands, replacements, names, mapper):\n",
    "\n",
    "    c = 0\n",
    "    for i, command in enumerate(commands):\n",
    "        if \"<args>\" in command:\n",
    "            if names[c] in list(mapper.keys()):\n",
    "                commands[i] = command.replace(\"<args>\", replacements[c])\n",
    "                c += 1\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da417c8",
   "metadata": {},
   "source": [
    "### Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9592497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a1mme\\AppData\\Local\\Temp\\ipykernel_19240\\3641845355.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['make preprocessor {\"preprocessor_name\":\"simple_imputer\",\"preprocessor_type\":\"imputer\",\"params\":{\"strategy\":\"mean\"}}']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"please make a single imputer\"\n",
    "# RAG output\n",
    "rag_output = rag_chain.invoke({\"question\": question})\n",
    "\n",
    "parsed_output = parse_command_list(rag_output)\n",
    "# Extract command names\n",
    "names = extract_keywords_hybrid(question, REFERENCE_KEYWORDS, top_n=len(parsed_output))\n",
    "# Extract args and types\n",
    "args_list = args_extractor(names, raw_mapping, data_mapping)\n",
    "# Parse RAG and replace placeholders\n",
    "final_output = replace_args(parsed_output, args_list, names, raw_mapping)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bdaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
