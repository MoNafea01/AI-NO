{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c47df9",
   "metadata": {},
   "source": [
    "### Imports & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ee1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai\n",
    "# !pip install langchain_community\n",
    "# !pip install langchain_huggingface\n",
    "# !pip install gradio\n",
    "# !pip install rapidfuzz\n",
    "# !pip install pypdf\n",
    "# !pip install faiss-cpu\n",
    "# !pip install spacy\n",
    "# !pip install fuzzywuzzy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install django\n",
    "# !pip install django-cors-headers\n",
    "# !pip install djangorestframework\n",
    "# !pip install drf_spectacular\n",
    "# !pip install psycopg2\n",
    "# !python.exe -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8afbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MO\\Depi\\envai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MO\\Depi\\envai\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, os, re, ast\n",
    "import subprocess\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "from rapidfuzz import fuzz\n",
    "pdf_file = \"./res/Cli script guidebook.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b9b56",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a640a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name=\"gemini-1.5-pro\"):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=500,\n",
    "        google_api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770839c8",
   "metadata": {},
   "source": [
    "#### Load API KEY from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "848913a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_path = './credentials/google_credentials.json'\n",
    "if not os.path.exists(api_key_path):\n",
    "    raise FileNotFoundError(f\"API key file not found at {api_key_path}. Please provide the correct path.\")\n",
    "\n",
    "with open(api_key_path, 'r') as f:\n",
    "    api_key = json.load(f)\n",
    "os.environ['GOOGLE_API_KEY'] = api_key.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ff185",
   "metadata": {},
   "source": [
    "### Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "56dac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_file)\n",
    "pdf_docs = loader.load_and_split()\n",
    "\n",
    "with open(\"res/data_mapping.json\", \"r\") as f:\n",
    "    data_mapping = json.load(f)\n",
    "\n",
    "REFERENCE_KEYWORDS = list(data_mapping.keys())\n",
    "data_mapping_doc = Document(page_content='Default Arguments for each node:\\n'+ json.dumps(data_mapping))\n",
    "\n",
    "all_docs = pdf_docs  + [data_mapping_doc]\n",
    "# print(all_docs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42781df",
   "metadata": {},
   "source": [
    "### Create Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "61295ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=all_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# vectorstore = FAISS.from_documents(documents=mapping_doc, embedding=embeddings)\n",
    "# mapping_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19f4b3",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c696a0",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "70c692a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def combine_inputs(input_dict, retriever):\n",
    "    question = input_dict[\"question\"]\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    return {\n",
    "        \"context\": format_docs(retrieved_docs),\n",
    "        \"question\": question\n",
    "    }\n",
    "\n",
    "def parse_command_list(output: str):\n",
    "    pattern = r\"\\[(.*?)\\]\"\n",
    "    matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        output = f\"[{matches[0]}]\"\n",
    "\n",
    "    try:\n",
    "        command_list = ast.literal_eval(output.strip())\n",
    "        return command_list if isinstance(command_list, list) else [command_list]\n",
    "    except Exception as e:\n",
    "        return [f\"Failed to parse: {e}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0452d50",
   "metadata": {},
   "source": [
    "#### LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dadf03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"You are an expert assistant trained to extract exact command-line instructions from a user guide.\n",
    "\n",
    "You are given the folliwing information:\n",
    "- A user guide that contains command-line instructions for a CLI tool.\n",
    "- all possible values for node_name\n",
    "- mapping of node names to their default arguments.\n",
    "\n",
    "Reference Information:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Carefully read the Reference Information, node names, and default arguments for each node.\n",
    "- Return the CLI command(s) that are relevant to the user's query.\n",
    "- Format your response as a raw Python list: ['command1', 'command2', ...]\n",
    "- Do NOT include any explanations, comments, or formatting outside the list.\n",
    "- If no command matches the query, return an empty list: []\n",
    "- The output MUST be ordered in the same order as the input.\n",
    "\n",
    "Example Query:\n",
    "I want to make a logistic regression model and make a project with id 3.\n",
    "\n",
    "Expected Response:\n",
    "[\n",
    "    'make logistic_regression <args>',\n",
    "    'create_project 3'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough()\n",
    "    | (lambda x: combine_inputs(x, retriever))\n",
    "    | prompt\n",
    "    | get_llm(model_name=\"gemini-2.0-flash\")\n",
    "    | StrOutputParser()\n",
    "    | parse_command_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177048a",
   "metadata": {},
   "source": [
    "### Keyword Extraction (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_extractor = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "keyword_embeddings = name_extractor.encode(REFERENCE_KEYWORDS, convert_to_tensor=True)\n",
    "\n",
    "def extract_keywords_hybrid(user_input: str, reference_keywords, top_n, transformer_thresh=0.7, fuzzy_thresh=80):\n",
    "    keyword_embeddings = name_extractor.encode(reference_keywords, convert_to_tensor=True)\n",
    "    doc = nlp(user_input.lower())\n",
    "    phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    matched_keywords = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase_embedding = name_extractor.encode(phrase, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(phrase_embedding, keyword_embeddings)[0]\n",
    "\n",
    "        for idx, score in enumerate(cosine_scores):\n",
    "            if score >= transformer_thresh:\n",
    "                if reference_keywords[idx] not in matched_keywords:\n",
    "                    matched_keywords.append(reference_keywords[idx])\n",
    "\n",
    "        # === Fuzzy matching fallback ===\n",
    "        for keyword in reference_keywords:\n",
    "            if fuzz.ratio(phrase, keyword.replace(\"_\", \" \")) >= fuzzy_thresh:\n",
    "                if keyword not in matched_keywords:\n",
    "                    matched_keywords.append(keyword)\n",
    "    top_n = min(top_n, len(matched_keywords))\n",
    "    best_match = matched_keywords.copy()\n",
    "    if len(matched_keywords) > top_n:\n",
    "        best_match = sorted(matched_keywords, key=lambda x: fuzz.ratio(user_input, x), reverse=True)[:top_n]\n",
    "    matched_keywords = list(filter(lambda x: x in best_match, matched_keywords))\n",
    "    return matched_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef6191",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3d4dd0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_extractor(names, reference_keywords, data_mapping):\n",
    "    result = []\n",
    "    for name in names:\n",
    "        if name in reference_keywords and name in data_mapping:\n",
    "            # node_type = mapping[name]\n",
    "            args_str = json.dumps(data_mapping[name], separators=(',', ':'))\n",
    "            result.append(args_str)\n",
    "    return result\n",
    "\n",
    "def replace_args(commands, replacements, names, reference_keywords):\n",
    "\n",
    "    c = 0\n",
    "    for i, command in enumerate(commands):\n",
    "        if \"<args>\" in command:\n",
    "            if names[c] in reference_keywords:\n",
    "                commands[i] = command.replace(\"<args>\", replacements[c])\n",
    "                c += 1\n",
    "\n",
    "    return commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"You are great at extracting suitable commands from user queries.\n",
    "\n",
    "# You are given:\n",
    "# - A mapping of node names to their default arguments.\n",
    "# - A user's natural language query.\n",
    "# - An empty list of commands to be filled.\n",
    "\n",
    "# Your task:\n",
    "# 1. Carefully read the user query.\n",
    "# 2. Identify any mentioned node names, even if they are misspelled or partially named (e.g., \"logistice\" â†’ \"logistic_regression\").\n",
    "# 3. For each recognized node:\n",
    "#    - Retrieve its default arguments from the mapping.\n",
    "#    - If the user provided new values for any arguments, update the default arguments accordingly.\n",
    "# 4. Format a command for each node in the following format:\n",
    "#    - `\"make <node_name> <arg1>=<value1> <arg2>=<value2> ...\"`\n",
    "#    - Skip arguments if they are empty.\n",
    "# 5. Return your response as a **raw Python list**, like: `['make node1 arg1=val1', 'make node2']`\n",
    "# 6. If no node matches the query, return an empty list: `[]`\n",
    "# 7. Do NOT explain or add any extra text. Only output the list.\n",
    "\n",
    "# Reference Mapping:\n",
    "# {context}\n",
    "\n",
    "# User Query:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "# prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d700f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_chain = (\n",
    "#     RunnablePassthrough()\n",
    "#     | (lambda x: combine_inputs(x, mapping_retriever))\n",
    "#     | prompt\n",
    "#     | get_llm(model_name=\"gemini-2.0-flash\")\n",
    "#     | StrOutputParser()\n",
    "#     | parse_command_list\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da417c8",
   "metadata": {},
   "source": [
    "### Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c9592497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG:  ['create_project 3', 'make logistic_regression <args>', 'make standard_scaler <args>']\n",
      "Extracted nodes' names:  ['logistic_regression', 'standard_scaler']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['create_project 3',\n",
       " 'make logistic_regression {\"model_name\":\"logistic_regression\",\"task\":\"classification\",\"model_type\":\"linear_models\",\"params\":{\"penalty\":\"l2\",\"C\":1.0}}',\n",
       " 'make standard_scaler {\"preprocessor_name\":\"standard_scaler\",\"preprocessor_type\":\"scaler\",\"params\":{\"with_mean\":true,\"with_std\":true}}']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"I want to create a project that has id of 3, use logistice regression model and use standard scaler\"\n",
    "# RAG output\n",
    "rag_output = rag_chain.invoke({\"question\": question})\n",
    "print(\"RAG: \",rag_output)\n",
    "# # Extract command names\n",
    "names = extract_keywords_hybrid(question, REFERENCE_KEYWORDS, top_n=len(rag_output))\n",
    "print(\"Extracted nodes' names: \",names)\n",
    "# # Extract args and types\n",
    "args_list = args_extractor(names, REFERENCE_KEYWORDS, data_mapping)\n",
    "# # Parse RAG and replace placeholders\n",
    "final_output = replace_args(rag_output, args_list, names, REFERENCE_KEYWORDS)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bdaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
